import re
import argparse
import csv
from pathlib import Path

import cv2
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
from torch.utils.data import DataLoader
from torch.utils.data.dataloader import default_collate

import jax.numpy as jnp
import orbax.checkpoint as ocp
import albumentations as A
from flax import nnx
from jax.tree_util import tree_map

from models.resnet import resnet18


class Config:
    """é›†ä¸­ç®¡ç†å¸¸é‡å‚æ•°"""
    TARGET_SIZE = 224
    NUM_CLASSES = 6
    BATCH_SIZE = 32
    NUM_WORKERS = 2
    CHECKPOINT_DIR = Path("/Users/billy/Documents/DLStudy/JaxVision/checkpoints")
    CLASS_NAMES = [
        "Chickenpox",
        "Cowpox",
        "Healthy",
        "HFMD",
        "Measles",
        "Monkeypox",
    ]
    VALID_EXTENSIONS = (".jpg", ".jpeg", ".png", ".bmp", ".tiff")


def numpy_collate(batch):
    """
    Custom collate: PyTorch tensors -> numpy arrays (for JAX)
    """
    # default_collate ä¼šæŠŠæ‰€æœ‰å¼ é‡å˜æˆä¸€ä¸ª batch çš„ tensor
    # ç„¶åæˆ‘ä»¬ç”¨ tree_map æŠŠ Tensor -> numpy
    return tree_map(lambda x: x.numpy() if isinstance(x, torch.Tensor) else x,
                    default_collate(batch))


def load_image_rgb(img_path: Path) -> np.ndarray:
    """
    Load image in RGB format. å¦‚æœè¯»ä¸åˆ°ï¼ŒæŠ›å‡ºå¼‚å¸¸ã€‚
    """
    img = cv2.imread(str(img_path))
    if img is None:
        raise FileNotFoundError(f"æ— æ³•è¯»å–å›¾ç‰‡: {img_path}")
    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)


def build_test_transform(target_size: int) -> A.Compose:
    """
    è¿”å›ä»…åŒ…å« Resize å’Œ Normalize çš„æµ‹è¯•/æ¨ç†æ—¶ç”¨çš„ Albumentations pipeline
    """
    return A.Compose([
        A.Resize(height=target_size, width=target_size, p=1.0),
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225],
            max_pixel_value=255.0,
        ),
    ])


class PredictionDataset(torch.utils.data.Dataset):
    """
    æ•°æ®é›†ï¼šéå†ä¸€ä¸ªç›®å½•ä¸‹æ‰€æœ‰å›¾ç‰‡ï¼ŒåŠ è½½ã€åº”ç”¨ transform å¹¶è¿”å› (numpy_image, path, name)
    """

    def __init__(self, image_dir: Path, transforms: A.Compose = None):
        self.image_dir = image_dir
        self.transforms = transforms

        # ç”¨ rglob ä¸€æ¬¡æ€§åŒ¹é…æ‰€æœ‰åç¼€ (å¤§å°å†™)
        self.image_paths = []
        for ext in Config.VALID_EXTENSIONS:
            pattern = f"*{ext}"
            self.image_paths += list(image_dir.rglob(pattern))
            self.image_paths += list(image_dir.rglob(pattern.upper()))

        # ä¿è¯é¡ºåºä¸€è‡´
        self.image_paths = sorted(set(self.image_paths))
        if not self.image_paths:
            raise ValueError(f"ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå›¾ç‰‡: {image_dir}")

        print(f"ğŸ—‚ï¸ å…±æ‰¾åˆ° {len(self.image_paths)} å¼ å›¾ç‰‡: {image_dir}")

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        path = self.image_paths[idx]
        img = load_image_rgb(path)
        if self.transforms:
            img = self.transforms(image=img)["image"]
        # è¿”å› (numpy_image, str(path), image_name)
        return img, str(path), path.name


def create_dataloader(image_dir: str, batch_size: int, num_workers: int, target_size: int):
    """
    ç›´æ¥åˆ›å»º Dataset + DataLoader
    """
    tfm = build_test_transform(target_size)
    dataset = PredictionDataset(Path(image_dir), transforms=tfm)
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=numpy_collate,
        num_workers=num_workers,
        pin_memory=True,
    )
    return loader


def create_model(seed: int, num_classes: int):
    """
    JAX + Flax ä¸‹åˆ›å»º ResNet18 æ¨¡å‹å®ä¾‹ï¼ˆå‚æ•° shape ç”¨äºåˆå§‹åŒ–ï¼‰
    """
    return resnet18(rngs=nnx.Rngs(seed), num_classes=num_classes)


def load_model_from_checkpoint(ckpt_path: str, num_classes: int):
    """
    ä» checkpoint æ¢å¤æ¨¡å‹å‚æ•°ï¼Œå¹¶åˆ‡æ¢åˆ° eval æ¨¡å¼
    """
    # å…ˆç”¨ eval_shape æ„é€ ä¸€ä¸ªåŒç»“æ„çš„â€œç©ºâ€ model
    model = nnx.eval_shape(lambda: create_model(0, num_classes))
    state = nnx.state(model)

    checkpointer = ocp.PyTreeCheckpointer()
    state = checkpointer.restore(ckpt_path, item=state)
    nnx.update(model, state)
    model.eval()
    return model




def find_best_checkpoint(checkpoint_dir: Path) -> str:
    """
    åœ¨checkpointç›®å½•é‡Œæ‰¾å¸¦ 'best_model' ä¸” acc æœ€å¤§çš„æ–‡ä»¶å¤¹ï¼Œ
    è¿”å›è¯¥æ–‡ä»¶å¤¹ä¸‹çš„ 'state' è·¯å¾„
    """
    if not checkpoint_dir.exists():
        raise ValueError(f"Checkpoint ç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")

    # æ‰€æœ‰åå­—åŒ…å« best_model çš„å­ç›®å½•
    candidates = [d for d in checkpoint_dir.iterdir() if d.is_dir() and "best_model" in d.name]
    if not candidates:
        raise ValueError(f"æ²¡æ‰¾åˆ° any 'best_model' å­ç›®å½•: {checkpoint_dir}")

    # ç”¨æ­£åˆ™æå– â€œAcc_0.952000â€ é‡Œé¢çš„å°æ•°
    def extract_acc(name: str) -> float:
        m = re.search(r"Acc_([0-9]*\.?[0-9]+)", name)
        return float(m.group(1)) if m else 0.0

    # æŒ‰ acc é™åºæ’åˆ—
    candidates.sort(key=lambda d: extract_acc(d.name), reverse=True)
    best_dir = candidates[0]
    state_file = best_dir / "state"
    print(f"ğŸ¯ é€‰æ‹© checkpoint: {best_dir.name}")
    return str(state_file)


def preprocess_one(image_path: str, transform: A.Compose):
    """
    å•å¼ å›¾ç‰‡é¢„å¤„ç†: è¯»å›¾ -> å˜æ¢ -> æ‰©å±• batch ç»´åº¦ -> è½¬ JAX array
    """
    img = load_image_rgb(Path(image_path))
    img_t = transform(image=img)["image"]
    # è½¬ä¸º float32 å¹¶åœ¨ axis=0 æ·»åŠ  batch ç»´åº¦
    return jnp.expand_dims(jnp.array(img_t, dtype=jnp.float32), axis=0), img


@nnx.jit
def predict_batch(model, images: jnp.ndarray):
    """
    JIT ç¼–è¯‘çš„é¢„æµ‹æ¥å£: 
    è¾“å…¥: (batch, H, W, C)
    è¿”å›: (pred_idx, å…¨æ¦‚ç‡åˆ†å¸ƒ)
    """
    logits = model(images)
    probs = nnx.softmax(logits, axis=-1)
    preds = jnp.argmax(logits, axis=-1)
    return preds, probs


def predict_single(model, image_path: str, transform: A.Compose, class_names: list):
    """
    å•å¼ å›¾ç‰‡æ¨ç†ï¼Œè¿”å› dict åŒ…å«é¢„æµ‹ç»“æœã€åŸå›¾
    """
    img_batch, orig_img = preprocess_one(image_path, transform)
    pred_idx, probs = predict_batch(model, img_batch)

    idx = int(pred_idx[0])
    return {
        "pred_idx": idx,
        "pred_name": class_names[idx],
        "confidence": float(probs[0, idx]),
        "all_probs": np.array(probs[0]),
        "orig_img": orig_img,
        "image_path": image_path,
        "image_name": Path(image_path).name,
    }


def predict_directory(model, image_dir: str, class_names: list,
                      batch_size: int, num_workers: int, save_csv: bool = False):
    """
    å¯¹ç›®å½•ä¸‹æ‰€æœ‰å›¾ç‰‡è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨ DataLoader æ‰¹é‡å¤„ç†
    """
    print(f"\nğŸ—‚ï¸ å¯¹ç›®å½•è¿›è¡Œæ¨ç†: {image_dir}")
    loader = create_dataloader(image_dir, batch_size, num_workers, Config.TARGET_SIZE)

    all_results = []
    for images_np, paths, names in tqdm(loader, desc="æ‰¹é‡æ¨ç†ä¸­"):
        # æ ·æœ¬å·²ç»æ˜¯ numpy-arrayï¼Œshape=(batch, H, W, C)
        # ç›´æ¥è½¬ JAX array
        images_jax = jnp.array(images_np, dtype=jnp.float32)
        preds, probs = predict_batch(model, images_jax)

        # éå† batch å†…çš„æ¯å¼ å›¾ï¼Œæ”¶é›†ç»“æœ
        for i in range(len(paths)):
            idx = int(preds[i])
            all_results.append({
                "image_path": paths[i],
                "image_name": names[i],
                "pred_idx": idx,
                "pred_name": class_names[idx],
                "confidence": float(probs[i, idx]),
                "all_probs": np.array(probs[i]),
            })

    _print_summary(all_results)
    if save_csv:
        _save_results_csv(all_results, image_dir, class_names)
    return all_results


def _print_summary(results: list):
    """
    æ‰“å°é¢„æµ‹æ€»ç»“ä¿¡æ¯ï¼šæ€»æ•°ã€å¹³å‡ç½®ä¿¡åº¦ã€å„ç±»åˆ†å¸ƒ & æœ€é«˜/æœ€ä½ç½®ä¿¡æ ·æœ¬
    """
    total = len(results)
    if total == 0:
        print("âš ï¸ æ²¡æœ‰ç»“æœå¯å±•ç¤º")
        return

    avg_conf = sum(r["confidence"] for r in results) / total
    print("\nğŸ“Š é¢„æµ‹æ€»ç»“ï¼š")
    print(f"  æ€»å›¾åƒæ•°é‡: {total}")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.4f}")

    # å„ç±»è®¡æ•°
    class_counts = {}
    for r in results:
        class_counts[r["pred_name"]] = class_counts.get(r["pred_name"], 0) + 1

    print("  ç±»åˆ«åˆ†å¸ƒï¼š")
    for cls, cnt in sorted(class_counts.items()):
        pct = cnt / total * 100
        print(f"    {cls}: {cnt} ({pct:.1f}%)")

    # æ’åºæ‰¾æœ€é«˜ / æœ€ä½
    sorted_r = sorted(results, key=lambda x: x["confidence"], reverse=True)
    print("\nğŸ¯ æœ€é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼š")
    for i, r in enumerate(sorted_r[:3], 1):
        print(f"    {i}. {r['image_name']}: {r['pred_name']} ({r['confidence']:.4f})")

    print("\nğŸ¤” æœ€ä½ç½®ä¿¡åº¦æ ·æœ¬ï¼š")
    for i, r in enumerate(sorted_r[-3:], 1):
        print(f"    {i}. {r['image_name']}: {r['pred_name']} ({r['confidence']:.4f})")




def _save_results_csv(results: list, image_dir: str, class_names: list):
    """
    å°†é¢„æµ‹ç»“æœä¿å­˜åˆ° CSV æ–‡ä»¶ï¼ŒåŒ…å«æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ—
    """
    out_path = Path(image_dir) / "prediction_results.csv"
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {out_path}")

    fieldnames = ["image_name", "image_path", "pred_name", "confidence"]
    # æ·»åŠ æ¯ä¸ª class çš„æ¦‚ç‡åˆ—
    for c in class_names:
        fieldnames.append(f"prob_{c}")

    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for r in results:
            row = {
                "image_name": r["image_name"],
                "image_path": r["image_path"],
                "pred_name": r["pred_name"],
                "confidence": f"{r['confidence']:.6f}",
            }
            for idx, cn in enumerate(class_names):
                row[f"prob_{cn}"] = f"{r['all_probs'][idx]:.6f}"
            writer.writerow(row)

    print("âœ… ä¿å­˜å®Œæˆï¼")


def visualize_results_sample(results: list, num_samples: int = 9):
    """
    ä»æœ€é«˜ä¸æœ€ä½ç½®ä¿¡åº¦å„å–éƒ¨åˆ†æ ·æœ¬ï¼Œæ˜¾ç¤ºå›¾ + é¢„æµ‹ç»“æœ
    """
    if not results:
        print("âš ï¸ æ— ç»“æœå¯è§†åŒ–")
        return

    sorted_r = sorted(results, key=lambda x: x["confidence"], reverse=True)
    top_k = sorted_r[: num_samples // 2]
    low_k = sorted_r[-(num_samples - num_samples // 2):]
    samples = top_k + low_k

    cols = 3
    rows = (len(samples) + cols - 1) // cols
    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))

    # ç¡®ä¿ axes æ˜¯äºŒç»´æ•°ç»„å½¢å¼
    if rows == 1:
        axes = axes.reshape(1, -1)
    elif cols == 1:
        axes = axes.reshape(-1, 1)

    for i, r in enumerate(samples):
        row, col = divmod(i, cols)
        ax = axes[row, col]
        try:
            img = load_image_rgb(Path(r["image_path"]))
            ax.imshow(img)
            title = f"{r['image_name']}\n{r['pred_name']}\nConf: {r['confidence']:.3f}"
            ax.set_title(title, fontsize=10)
            ax.axis("off")
        except Exception:
            ax.text(0.5, 0.5, f"åŠ è½½å¤±è´¥\n{r['image_name']}", ha="center", va="center", transform=ax.transAxes)
            ax.set_title(f"{r['pred_name']}\nConf: {r['confidence']:.3f}")
            ax.axis("off")

    # éšè—å¤šä½™å­å›¾
    total_plots = rows * cols
    for i in range(len(samples), total_plots):
        row, col = divmod(i, cols)
        axes[row, col].axis("off")

    plt.suptitle("ç¤ºä¾‹é¢„æµ‹ç»“æœï¼ˆé«˜/ä½ç½®ä¿¡åº¦ï¼‰", fontsize=14)
    plt.tight_layout()
    plt.show()


def visualize_prediction(result: dict, show_prob: bool = True):
    """
    å•å¼ å›¾ç‰‡å¯è§†åŒ–ï¼šå›¾åƒ + æ¡å½¢å›¾æ¦‚ç‡åˆ†å¸ƒ
    """
    figsize = (15, 6) if show_prob else (8, 6)
    fig, axs = plt.subplots(1, 2 if show_prob else 1, figsize=figsize)
    if not show_prob:
        axs = [axs]

    # å·¦å›¾ï¼šåŸå›¾ + æ ‡é¢˜
    img = result.get("orig_img", load_image_rgb(Path(result["image_path"])))
    axs[0].imshow(img)
    axs[0].set_title(f"{result['image_name']}\nPred: {result['pred_name']}\nConf: {result['confidence']:.4f}")
    axs[0].axis("off")

    if show_prob:
        # å³å›¾ï¼šæ¡å½¢å›¾è¡¨ç¤ºå„ç±»åˆ«æ¦‚ç‡
        classes = Config.CLASS_NAMES
        probs = result["all_probs"]
        bars = axs[1].bar(range(len(classes)), probs)
        axs[1].set_xticks(range(len(classes)))
        axs[1].set_xticklabels(classes, rotation=45, ha="right")
        axs[1].set_xlabel("ç±»åˆ«")
        axs[1].set_ylabel("æ¦‚ç‡")
        axs[1].set_title("å„ç±»æ¦‚ç‡åˆ†å¸ƒ")
        # é«˜äº®é¢„æµ‹ç±»åˆ«
        bars[result["pred_idx"]].set_color("red")

    plt.tight_layout()
    plt.show()


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨è®­ç»ƒå¥½çš„ ResNet18 æ¨¡å‹è¿›è¡Œæ¨ç†")
    parser.add_argument("--image", type=str, help="å•å¼ å›¾ç‰‡è·¯å¾„ï¼Œç”¨äºé¢„æµ‹")
    parser.add_argument("--images", type=str, nargs="+", help="å¤šå¼ å›¾ç‰‡è·¯å¾„åˆ—è¡¨")
    parser.add_argument("--directory", type=str, help="å›¾ç‰‡ç›®å½•ï¼Œç”¨äºæ‰¹é‡é¢„æµ‹")
    parser.add_argument("--checkpoint", type=str, help="æŒ‡å®š checkpoint æ–‡ä»¶ (å¯é€‰)")
    parser.add_argument("--checkpoint_dir", type=str, default=str(Config.CHECKPOINT_DIR),
                        help="checkpoint æ‰€åœ¨ç›®å½•")
    parser.add_argument("--batch_size", type=int, default=Config.BATCH_SIZE, help="æ‰¹é‡é¢„æµ‹æ—¶çš„ batch size")
    parser.add_argument("--num_workers", type=int, default=Config.NUM_WORKERS, help="DataLoader çš„ num_workers")
    parser.add_argument("--save_results", action="store_true", help="æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœåˆ° CSV")
    parser.add_argument("--no_viz", action="store_true", help="è·³è¿‡å¯è§†åŒ–")
    parser.add_argument("--class_names", type=str, nargs="+", default=Config.CLASS_NAMES, help="ç±»åˆ«åç§°åˆ—è¡¨")

    args = parser.parse_args()

    # è‡³å°‘æä¾›ä¸€ä¸ªè¾“å…¥é€‰é¡¹
    if not (args.image or args.images or args.directory):
        print("âŒ è¯·é€‰æ‹© --image / --images / --directory ä¸­çš„ä¸€ç§å‚æ•°æ¥æ‰§è¡Œé¢„æµ‹")
        return

    print("ğŸ”® å¼€å§‹é¢„æµ‹æµç¨‹...")

    # é€‰æ‹© checkpoint
    ckpt_path = args.checkpoint
    if not ckpt_path:
        try:
            ckpt_path = find_best_checkpoint(Path(args.checkpoint_dir))
        except Exception as e:
            print(f"âŒ è·å–æœ€ä½³ checkpoint å¤±è´¥: {e}")
            return

    # åŠ è½½æ¨¡å‹
    print("\nğŸ—ï¸ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    try:
        model = load_model_from_checkpoint(ckpt_path, Config.NUM_CLASSES)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å‡ºé”™: {e}")
        return

    # ç»Ÿä¸€ transform
    transform = build_test_transform(Config.TARGET_SIZE)

    # å•å¼ å›¾ç‰‡
    if args.image:
        print(f"\nğŸ” æ­£åœ¨å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œé¢„æµ‹: {args.image}")
        try:
            res = predict_single(model, args.image, transform, args.class_names)
            print("\nğŸ“Š é¢„æµ‹ç»“æœï¼š")
            print(f"  å›¾åƒ: {res['image_path']}")
            print(f"  é¢„æµ‹ç±»åˆ«: {res['pred_name']}")
            print(f"  ç½®ä¿¡åº¦: {res['confidence']:.4f}")
            print(f"  ç±»åˆ«ç´¢å¼•: {res['pred_idx']}")

            if not args.no_viz:
                visualize_prediction(res)
        except Exception as e:
            print(f"âŒ å•å›¾é¢„æµ‹å¤±è´¥: {e}")

    # å¤šå¼ å›¾ç‰‡é€ä¸€é¢„æµ‹
    elif args.images:
        print(f"\nğŸ” æ­£åœ¨å¯¹å¤šå¼ å›¾ç‰‡åˆ—è¡¨è¿›è¡Œé¢„æµ‹ï¼Œå…± {len(args.images)} å¼ ")
        results = []
        for img_path in args.images:
            try:
                r = predict_single(model, img_path, transform, args.class_names)
                results.append(r)
                print(f"âœ… {r['image_name']}: {r['pred_name']} ({r['confidence']:.4f})")
            except Exception as e:
                print(f"âŒ {Path(img_path).name} é¢„æµ‹å¤±è´¥: {e}")

        if results:
            print("\nğŸ“Š æ‰¹é‡é¢„æµ‹ç»Ÿè®¡ï¼š")
            dist = {}
            for r in results:
                dist[r["pred_name"]] = dist.get(r["pred_name"], 0) + 1
            for cls, cnt in dist.items():
                print(f"  {cls}: {cnt}")

            if not args.no_viz and results:
                print("ğŸ” æ˜¾ç¤ºç¬¬ä¸€å¼ é¢„æµ‹ç»“æœå¯è§†åŒ–")
                visualize_prediction(results[0])

    # æ•´ä¸ªç›®å½•
    elif args.directory:
        print(f"\nğŸ” æ­£åœ¨å¯¹ç›®å½•è¿›è¡Œæ‰¹é‡é¢„æµ‹: {args.directory}")
        try:
            all_results = predict_directory(
                model,
                args.directory,
                args.class_names,
                args.batch_size,
                args.num_workers,
                save_csv=args.save_results
            )
            if not args.no_viz and all_results:
                print("\nğŸ“ˆ å±•ç¤ºç¤ºä¾‹å¯è§†åŒ–")
                visualize_results_sample(all_results)
        except Exception as e:
            print(f"âŒ ç›®å½•é¢„æµ‹å‡ºé”™: {e}")
            return


if __name__ == "__main__":
    main()
